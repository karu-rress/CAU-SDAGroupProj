{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvHFDLdaWRC+LDY4zCbsd6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karu-rress/SDAGroupProj/blob/main/0aj_TreeForestLogistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "co_j5lkps2Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fle5rBM1oD3F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "import shap\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_origin = pd.read_csv('/content/Android_Malware.csv', low_memory=False)"
      ],
      "metadata": {
        "id": "Yyp6e1XPqohJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_origin.copy() # Just in case ;)"
      ],
      "metadata": {
        "id": "FdcDExFiqy6P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()"
      ],
      "metadata": {
        "id": "SRf656-0q7Aq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pairs of columns for differences\n",
        "packet_pairs = [('Total Fwd Packets', 'Total Backward Packets'),\n",
        "                ('Total Length of Fwd Packets', 'Total Length of Bwd Packets'),\n",
        "                ('Fwd IAT Total', 'Bwd IAT Total'),\n",
        "                ('Fwd PSH Flags', 'Bwd PSH Flags'),\n",
        "                ('Fwd URG Flags', 'Bwd URG Flags'),\n",
        "                ('Fwd Header Length', 'Bwd Header Length'),\n",
        "                ('Fwd Packets/s', 'Bwd Packets/s'),\n",
        "                ('Avg Fwd Segment Size', 'Avg Bwd Segment Size'),\n",
        "                ('Fwd Avg Bytes/Bulk', 'Bwd Avg Bytes/Bulk'),\n",
        "                ('Fwd Avg Packets/Bulk', 'Bwd Avg Packets/Bulk'),\n",
        "                ('Fwd Avg Bulk Rate', 'Bwd Avg Bulk Rate'),\n",
        "                ('Subflow Fwd Packets', 'Subflow Bwd Packets'),\n",
        "                ('Subflow Fwd Bytes', 'Subflow Bwd Bytes'),\n",
        "                ('Init_Win_bytes_forward', 'Init_Win_bytes_backward')]\n",
        "\n",
        "# Convert columns to numeric before calculating differences\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Calculate differences for each pair and create separate columns\n",
        "for pair in packet_pairs:\n",
        "    col_name_diff = f'{pair[0]} - {pair[1]}'\n",
        "    df[col_name_diff] = df[pair[0]] - df[pair[1]]"
      ],
      "metadata": {
        "id": "noeVMmpZq_kp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all columns to drop\n",
        "columns_to_drop = [\n",
        "    'Unnamed: 0', 'Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port',\n",
        "    'Protocol', 'Timestamp', 'Total Fwd Packets', 'Total Backward Packets',\n",
        "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Std',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Std',\n",
        "    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
        "    'Fwd IAT Total', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
        "    'Bwd IAT Total', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
        "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
        "    'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Min Packet Length', 'Max Packet Length', 'Packet Length Std', 'Packet Length Variance',\n",
        "    'ECE Flag Count', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
        "    'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
        "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate',\n",
        "    'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes',\n",
        "    'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "    'Active Std', 'Active Max', 'Active Min', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "# Drop the specified columns\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore')"
      ],
      "metadata": {
        "id": "21QxfEBurFc9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'] = df_origin['Label']"
      ],
      "metadata": {
        "id": "0KKfTjOCrNuz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_with_null = df.columns[df.isnull().any()]\n",
        "df_null_counts = df[columns_with_null].isnull().sum()\n",
        "print(df_null_counts[df_null_counts > 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiKR3R6JrYR-",
        "outputId": "4f3e8b73-7d53-43d0-ccf5-40b063e11dca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bwd Packet Length Mean                              1\n",
            "Flow Bytes/s                                        1\n",
            "Flow Packets/s                                      1\n",
            "Flow IAT Mean                                       1\n",
            "Fwd IAT Mean                                        1\n",
            "Bwd IAT Mean                                        1\n",
            "Packet Length Mean                                  1\n",
            "FIN Flag Count                                      1\n",
            "SYN Flag Count                                      1\n",
            "RST Flag Count                                      1\n",
            "PSH Flag Count                                      1\n",
            "ACK Flag Count                                      1\n",
            "URG Flag Count                                      1\n",
            "CWE Flag Count                                      1\n",
            "Down/Up Ratio                                       1\n",
            "Average Packet Size                                 1\n",
            "act_data_pkt_fwd                                    1\n",
            "min_seg_size_forward                                1\n",
            "Active Mean                                         1\n",
            "Idle Mean                                           1\n",
            "Label                                               1\n",
            "Fwd IAT Total - Bwd IAT Total                       1\n",
            "Fwd PSH Flags - Bwd PSH Flags                       1\n",
            "Fwd URG Flags - Bwd URG Flags                       1\n",
            "Fwd Header Length - Bwd Header Length               1\n",
            "Fwd Packets/s - Bwd Packets/s                       1\n",
            "Avg Fwd Segment Size - Avg Bwd Segment Size         1\n",
            "Fwd Avg Bytes/Bulk - Bwd Avg Bytes/Bulk             1\n",
            "Fwd Avg Packets/Bulk - Bwd Avg Packets/Bulk         1\n",
            "Fwd Avg Bulk Rate - Bwd Avg Bulk Rate               1\n",
            "Subflow Fwd Packets - Subflow Bwd Packets           1\n",
            "Subflow Fwd Bytes - Subflow Bwd Bytes               1\n",
            "Init_Win_bytes_forward - Init_Win_bytes_backward    1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_null_counts.shape\n",
        "#22 columns with null values. Drop them because it is very few"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElUBBuogr8Kd",
        "outputId": "f51f196c-44c7-4daa-c711-208c447dcc35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "rvfkH5NzsE9c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop(columns='Label')\n",
        "target = df['Label']"
      ],
      "metadata": {
        "id": "xgkldcaGsM_B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)\n"
      ],
      "metadata": {
        "id": "uVLlfqX9sUzT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "rWKrP2D1sehr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the random forest classifier\n",
        "forest = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(forest, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_forest = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best random forest model on the test set\n",
        "print(\"Best Random Forest Model:\")\n",
        "print(\"Training set accuracy: {:.3f}\".format(best_forest.score(X_train, y_train)))\n",
        "print(\"Test set accuracy: {:.3f}\".format(best_forest.score(X_test, y_test)))\n",
        "print(\"Best parameters: \", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "pyiXX_rXuaif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier(n_estimators=20, random_state=0)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set accuracy: {:.3f}\".format(forest.score(X_train, y_train)))\n",
        "print(\"Test set accuracy: {:.3f}\".format(forest.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpvWQU5sd_M",
        "outputId": "d0e4903a-4faa-4582-f23b-c7e0237e15a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set accuracy: 0.980\n",
            "Test set accuracy: 0.697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Specify the correct positive label for precision, recall, and f1 scores\n",
        "positive_label = 'Android_Adware'\n",
        "\n",
        "# Convert y_test to binary form for multi-class classification\n",
        "y_test_binary = label_binarize(y_test, classes=['Android_Adware', 'Android_Scareware'])\n",
        "\n",
        "# Define custom scorers for precision, recall, and f1\n",
        "precision_scorer = make_scorer(precision_score, average='macro')\n",
        "recall_scorer = make_scorer(recall_score, average='macro')\n",
        "f1_scorer = make_scorer(f1_score, average='macro')\n",
        "roc_auc_scorer = make_scorer(roc_auc_score, multi_class='ovr')\n",
        "\n",
        "# Use the custom scorers in cross_val_score with OneVsRestClassifier for multi-class\n",
        "forest_multi = OneVsRestClassifier(RandomForestClassifier())\n",
        "\n",
        "print(\"Default Cross-validation score:\", cross_val_score(forest_multi, X_test, y_test, scoring=\"accuracy\", cv=10))\n",
        "print(\"Accuracy score:\", cross_val_score(forest_multi, X_test, y_test, scoring=\"accuracy\", cv=10))\n",
        "print(\"Roc_Auc score:\", cross_val_score(forest_multi, X_test, y_test_binary, scoring=roc_auc_scorer, cv=10))\n",
        "print(\"Precision score:\", cross_val_score(forest_multi, X_test, y_test, scoring=precision_scorer, cv=10))\n",
        "print(\"Recall score:\", cross_val_score(forest_multi, X_test, y_test, scoring=recall_scorer, cv=10))\n",
        "print(\"f1 score:\", cross_val_score(forest_multi, X_test, y_test, scoring=f1_scorer, cv=10))"
      ],
      "metadata": {
        "id": "VK7ySgkQtk7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "Lr8e2iWD34xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Decision tree with pruning\n",
        "tree = DecisionTreeClassifier(random_state=0, max_depth=3, min_samples_split=10, min_samples_leaf=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Training set accuracy\n",
        "train_accuracy = accuracy_score(y_train, tree.predict(X_train))\n",
        "print(\"Training set accuracy: {:.3f}\".format(train_accuracy))\n",
        "\n",
        "# Test set accuracy\n",
        "test_accuracy = accuracy_score(y_test, tree.predict(X_test))\n",
        "print(\"Test set accuracy: {:.3f}\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg92jOyYuFby",
        "outputId": "48022863-2e7c-41d8-9cf5-9c8e8ec072ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set accuracy: 0.670\n",
            "Test set accuracy: 0.670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_tree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maO4hWQz4EIJ",
        "outputId": "6d759bcb-00a0-4d7d-a410-8a99214d515c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "   Android_Adware       0.67      0.99      0.80     36860\n",
            "Android_Scareware       0.56      0.02      0.04     18225\n",
            "\n",
            "         accuracy                           0.67     55085\n",
            "        macro avg       0.62      0.51      0.42     55085\n",
            "     weighted avg       0.63      0.67      0.55     55085\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "eBuUNwUu4MlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler to scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model with increased max_iter\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=0)\n",
        "\n",
        "# Fit the model to the scaled training data\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model on the scaled data\n",
        "training_accuracy = log_reg.score(X_train_scaled, y_train)\n",
        "test_accuracy = log_reg.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Training set accuracy: {:.3f}\".format(training_accuracy))\n",
        "print(\"Test set accuracy: {:.3f}\".format(test_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wXFv1rx4OwS",
        "outputId": "0f74893a-f352-48fe-ffa5-161e8a479d0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set accuracy: 0.668\n",
            "Test set accuracy: 0.668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnjJvowEmPUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}